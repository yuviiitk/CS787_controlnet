{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13693676,"sourceType":"datasetVersion","datasetId":8709938},{"sourceId":634789,"sourceType":"modelInstanceVersion","modelInstanceId":478609,"modelId":494455},{"sourceId":641493,"sourceType":"modelInstanceVersion","modelInstanceId":483779,"modelId":499290}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom diffusers import StableDiffusionPipeline\n\ndef get_device():\n    if torch.cuda.is_available():\n        print(f\"✅ CUDA device found: {torch.cuda.get_device_name(0)}\")\n        return torch.device(\"cuda\")\n    elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() and torch.backends.mps.is_built():\n        print(\"✅ MPS device found. Using Metal acceleration.\")\n        return torch.device(\"mps\")\n    else:\n        print(\"⚠️ No GPU acceleration found. Falling back to CPU.\")\n        return torch.device(\"cpu\")\n\nDEVICE = get_device()\n# DEVICE = torch.device(\"cpu\")\n\n# load SD model and sample\npipe = StableDiffusionPipeline.from_pretrained(\"/kaggle/input/stablediffusion5669/tensorflow2/default/1/models/sd-v1-5\", torch_dtype=torch.float32)\npipe.safety_checker = None\npipe.enable_attention_slicing()\npipe = pipe.to(DEVICE)\nprint(f\"✅ Model loaded to {DEVICE}.\")\n\nprompt = input(\"Enter your text prompt: \").strip()\nif not prompt:\n    prompt = \"a futuristic city skyline at sunset\"\n\nout = pipe(prompt, num_inference_steps=50, guidance_scale=7.5)\nout.images[0].save(\"sd_sample.png\")\nprint(f\"✅ Saved sd_sample.png using {DEVICE}.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T12:19:44.612047Z","iopub.execute_input":"2025-11-13T12:19:44.612949Z","iopub.status.idle":"2025-11-13T12:20:39.620166Z","shell.execute_reply.started":"2025-11-13T12:19:44.612921Z","shell.execute_reply":"2025-11-13T12:20:39.619436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !/usr/bin/env python3\n# ==== env BEFORE importing torch ====\nimport os\nos.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True,max_split_size_mb:64\")\n\nimport math, random, time\nfrom pathlib import Path\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.transforms import InterpolationMode\n\nfrom transformers import CLIPTextModel, CLIPTokenizer, get_cosine_schedule_with_warmup\nfrom diffusers import (\n    AutoencoderKL,\n    UNet2DConditionModel,\n    ControlNetModel,\n    DDPMScheduler,\n)\nfrom torch.optim import AdamW\n\n# ----------------- TUNED CONFIG (aim: lower loss, fewer epochs) -----------------\nMODEL_DIR_SD = \"/kaggle/input/stablediffusion5669/tensorflow2/default/1/models/sd-v1-5\"\nDATA_ROOT     = \"/kaggle/input/canny-tuples/dataset\"\nOUTPUT_DIR    = \"controlnet_scratch_out\"\n\nBATCH_SIZE  = 1\nGRAD_ACCUM  = 8                         # effective batch = 8\nEPOCHS      = 5                        # looped, but MAX_STEPS terminates\nMAX_STEPS   = 12000                     # steps target (stop early when hit)\nLR          = 1e-5\nWEIGHT_DECAY= 1e-4                      # milder regularization (was 1e-2)\nWARMUP_STEPS= 1000                      # warmup for quicker convergence\nIMAGE_SIZE  = 256\nSAVE_EVERY  = 500                       # save a bit more frequently\nSEED        = 42\nDROP_HINT_P = 0.0                       # disable until fidelity is good\nOVERFIT_ONE = None                      # e.g., \"00001.jpg\" to overfit/debug; else None\n\nDEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nUSE_FP16 = bool(torch.cuda.is_available())\nprint(f\"[STEP] DEVICE={DEVICE} FP16={USE_FP16}\")\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# ---------------- SEEDING & CUDA BACKENDS -----------------\ndef seed_everything(s=SEED):\n    random.seed(s); np.random.seed(s)\n    torch.manual_seed(s)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(s)\nseed_everything()\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.benchmark = True\n\n# ---------------- DATASET -----------------\nclass TripletDataset(Dataset):\n    \"\"\"\n    Expects:\n      root/images/*.jpg|png\n      root/canny/*.jpg|png (same filenames)\n      root/captions.txt -> \"<filename>\\t<caption>\" per line\n    \"\"\"\n    def __init__(self, root: str, image_size: int, overfit_one: str | None = None):\n        root = Path(root)\n        img_dir = root / \"images\"\n        cn_dir  = root / \"canny\"\n        if not img_dir.exists() or not cn_dir.exists():\n            raise FileNotFoundError(f\"Expected folders: {img_dir} and {cn_dir}\")\n\n        img_files = sorted([p for p in img_dir.iterdir() if p.suffix.lower() in (\".png\",\".jpg\",\".jpeg\")])\n        can_files = sorted([p for p in cn_dir.iterdir()  if p.suffix.lower() in (\".png\",\".jpg\",\".jpeg\")])\n\n        # --- robust caption mapping by filename ---\n        capf = root / \"captions.txt\"\n        caps_by_name = {}\n        if capf.exists():\n            with open(capf, \"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    line = line.strip()\n                    if not line: \n                        continue\n                    if \"\\t\" in line:\n                        fn, cap = line.split(\"\\t\", 1)\n                    else:\n                        fn, cap = line, line\n                    caps_by_name[fn.strip()] = cap.strip()\n\n        # filter to overfit-one if requested\n        if overfit_one is not None:\n            img_files = [p for p in img_files if p.name == overfit_one]\n            can_files = [p for p in can_files if p.name == overfit_one]\n            if len(img_files) == 0:\n                raise ValueError(f\"OVERFIT_ONE='{overfit_one}' not found in images/\")\n            # replicate sample to stabilize loader stats\n            img_files = img_files * 128\n            can_files = can_files * 128\n\n        assert len(img_files) == len(can_files), \"image and canny counts differ\"\n\n        self.img_files   = img_files\n        self.canny_files = can_files\n        self.captions    = [caps_by_name.get(p.name, \"\") for p in self.img_files]\n\n        assert len(self.captions) == len(self.img_files), \"captions count mismatch\"\n\n        self.tf_img = transforms.Compose([\n            transforms.Resize((image_size, image_size), interpolation=InterpolationMode.LANCZOS),\n            transforms.ToTensor(),  # [0,1]\n        ])\n        self.tf_hint = transforms.Compose([\n            transforms.Resize((image_size, image_size), interpolation=InterpolationMode.NEAREST),\n            transforms.ToTensor(),  # [0,1]\n        ])\n\n    def __len__(self): return len(self.img_files)\n\n    def __getitem__(self, i):\n        img = Image.open(self.img_files[i]).convert(\"RGB\")\n        cn  = Image.open(self.canny_files[i]).convert(\"L\")\n        img = self.tf_img(img)                 # [3,H,W]\n        cn  = self.tf_hint(cn).repeat(3,1,1)   # [1,H,W] -> [3,H,W]\n        return {\"image\": img, \"canny\": cn, \"prompt\": self.captions[i]}\n\n# ---------------- LOAD SD COMPONENTS -----------------\ndef load_sd(sd_dir: str, device: str):\n    tok = CLIPTokenizer.from_pretrained(str(Path(sd_dir) / \"tokenizer\"))\n    te  = CLIPTextModel.from_pretrained(str(Path(sd_dir) / \"text_encoder\"))\n    vae = AutoencoderKL.from_pretrained(sd_dir, subfolder=\"vae\")\n    un  = UNet2DConditionModel.from_pretrained(sd_dir, subfolder=\"unet\")\n\n    for m in (vae, un, te):\n        m.to(device).eval()\n        for p in m.parameters(): p.requires_grad = False\n    return tok, te, vae, un\n\ntokenizer, text_encoder, vae, unet = load_sd(MODEL_DIR_SD, DEVICE)\n\n# ControlNet from UNet (paper-consistent zero-conv init preserved)\ncontrolnet = ControlNetModel.from_unet(unet).to(DEVICE).train()\n\n# ---- Memory savers: gradient checkpointing + xFormers (optional) ----\ntry:\n    controlnet.enable_gradient_checkpointing()\nexcept Exception as e:\n    print(\"[WARN] grad checkpointing not available:\", e)\n\ntry:\n    import xformers  # noqa:F401\n    unet.enable_xformers_memory_efficient_attention()\n    controlnet.enable_xformers_memory_efficient_attention()\n    print(\"[OK] xFormers attention enabled.\")\nexcept Exception as e:\n    print(\"[WARN] xFormers not enabled:\", e)\n\n# ---- SD1.5 scheduler params (scaled_linear) ----\nnoise_scheduler = DDPMScheduler(\n    num_train_timesteps=1000,\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\"\n)\n\n# ---- DataLoader (drop_last keeps shapes stable) ----\nds  = TripletDataset(DATA_ROOT, IMAGE_SIZE, overfit_one=OVERFIT_ONE)\ndl  = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True,\n                 num_workers=0, pin_memory=True, drop_last=True)\nprint(f\"[OK] dataset={len(ds)} batches/epoch={len(dl)} overfit_one={OVERFIT_ONE}\")\n\n# ---- Optimizer + Cosine schedule with warmup ----\ndef make_optimizer(params, lr):\n    try:\n        import bitsandbytes as bnb  # pip install bitsandbytes\n        print(\"[OK] Using bitsandbytes AdamW8bit\")\n        return bnb.optim.AdamW8bit(params, lr=lr, betas=(0.9,0.999), eps=1e-8, weight_decay=WEIGHT_DECAY)\n    except Exception as e:\n        print(\"[WARN] 8-bit optimizer not available, using torch AdamW:\", e)\n        return AdamW(params, lr=lr, betas=(0.9,0.999), eps=1e-8, weight_decay=WEIGHT_DECAY)\n\nopt    = make_optimizer(controlnet.parameters(), LR)\nscaler = torch.cuda.amp.GradScaler(enabled=USE_FP16)\nmse    = nn.MSELoss()\n\n# total updates = MAX_STEPS, schedule steps move each optimizer step (not every batch)\nsched = get_cosine_schedule_with_warmup(\n    opt, num_warmup_steps=WARMUP_STEPS, num_training_steps=MAX_STEPS if MAX_STEPS else 12000\n)\n\n# ---------------- TRAIN -----------------\nglobal_step = 0\nstart = time.time()\nfor ep in range(EPOCHS):\n    if MAX_STEPS and global_step >= MAX_STEPS: break\n    pbar = tqdm(dl, desc=f\"Epoch {ep+1}\")\n    for step, batch in enumerate(pbar):\n        if MAX_STEPS and global_step >= MAX_STEPS: break\n\n        img  = batch[\"image\"].to(DEVICE)     # [0,1]\n        hint = batch[\"canny\"].to(DEVICE)     # [0,1]\n        prm  = batch[\"prompt\"]\n\n        # text embeddings (frozen)\n        with torch.no_grad():\n            tok_out = tokenizer(prm, padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\")\n            txt_ids = tok_out.input_ids.to(DEVICE)\n            txt_emb = text_encoder(txt_ids).last_hidden_state\n\n        # normalize to [-1,1] -> VAE -> latents\n        with torch.no_grad():\n            img_n = img * 2.0 - 1.0\n            lat   = vae.encode(img_n).latent_dist.sample() * 0.18215\n\n        # noise + timesteps\n        B  = lat.shape[0]\n        ts = torch.randint(0, noise_scheduler.num_train_timesteps, (B,), device=DEVICE).long()\n        n  = torch.randn_like(lat)\n        nlat = noise_scheduler.add_noise(lat, n, ts)\n\n        # hint preprocess + conditioning dropout (disabled for faster fidelity)\n        hint_n = hint * 2.0 - 1.0\n        if DROP_HINT_P > 0.0 and random.random() < DROP_HINT_P:\n            hint_n = torch.zeros_like(hint_n)\n\n        with torch.cuda.amp.autocast(enabled=USE_FP16):\n            cn_out = controlnet(\n                sample=nlat, timestep=ts, encoder_hidden_states=txt_emb,\n                controlnet_cond=hint_n, return_dict=True\n            )\n            pred = unet(\n                nlat, ts, encoder_hidden_states=txt_emb,\n                down_block_additional_residuals=cn_out.down_block_res_samples,\n                mid_block_additional_residual=cn_out.mid_block_res_sample,\n                return_dict=True\n            ).sample\n            loss = mse(pred, n) / GRAD_ACCUM\n\n        scaler.scale(loss).backward()\n\n        if (step + 1) % GRAD_ACCUM == 0:\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(controlnet.parameters(), 1.0)\n            scaler.step(opt); scaler.update()\n            sched.step()\n            opt.zero_grad(set_to_none=True)\n            global_step += 1\n\n            pbar.set_postfix({\"loss\": f\"{(loss.item()*GRAD_ACCUM):.4f}\", \"step\": global_step})\n            if global_step % 10 == 0:\n                print(f\"[PROGRESS] step {global_step} | loss={(loss.item()*GRAD_ACCUM):.4f} | lr={sched.get_last_lr()[0]:.2e}\")\n\n            if SAVE_EVERY and global_step % SAVE_EVERY == 0:\n                ck = Path(OUTPUT_DIR) / f\"ckpt_{global_step}\"\n                ck.mkdir(parents=True, exist_ok=True)\n                controlnet.cpu().save_pretrained(str(ck))\n                controlnet.to(DEVICE).train()\n                print(f\"[SAVE] checkpoint -> {ck}\")\n                torch.cuda.empty_cache()  # help fragmentation after save\n\n        if (step % 50) == 0:\n            torch.cuda.empty_cache()\n\n# final save\nfinal_dir = Path(OUTPUT_DIR) / \"controlnet_final\"\nfinal_dir.mkdir(parents=True, exist_ok=True)\ncontrolnet.cpu().save_pretrained(str(final_dir))\nprint(f\"[DONE] steps={global_step} time={(time.time()-start)/60:.1f} min | saved -> {final_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T10:13:25.354273Z","iopub.execute_input":"2025-11-13T10:13:25.354600Z","iopub.status.idle":"2025-11-13T11:41:21.792433Z","shell.execute_reply.started":"2025-11-13T10:13:25.354573Z","shell.execute_reply":"2025-11-13T11:41:21.791363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil, os\nfrom IPython.display import FileLink\n\n# You can change this if you want to zip an earlier checkpoint\nRUN_DIR = os.environ.get(\"CTRLNET_RUN_DIR\", \"/kaggle/working/controlnet_scratch_out\")\nZIP_OUT = \"/kaggle/working/controlnet_scratch_out.zip\"\n\n# Create zip file\nif not os.path.exists(RUN_DIR):\n    raise FileNotFoundError(f\"[ERR] run folder not found: {RUN_DIR}\")\n\n# Remove old zip if exists\ntry:\n    if os.path.exists(ZIP_OUT):\n        os.remove(ZIP_OUT)\nexcept Exception:\n    pass\n\nshutil.make_archive(ZIP_OUT.replace(\".zip\",\"\"), \"zip\", RUN_DIR)\nprint(f\"✅ Zipped -> {ZIP_OUT}\")\n\n# Kaggle download link\nFileLink(ZIP_OUT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T11:43:26.661402Z","iopub.execute_input":"2025-11-13T11:43:26.662252Z","iopub.status.idle":"2025-11-13T11:49:58.007835Z","shell.execute_reply.started":"2025-11-13T11:43:26.662222Z","shell.execute_reply":"2025-11-13T11:49:58.007158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nStableDiffusionControlNetPipeline inference using trained ControlNet.\nTweaks: match train resolution, moderate CFG, enable xFormers if available.\n\"\"\"\n\nimport os\nfrom pathlib import Path\nimport torch\nfrom PIL import Image\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom diffusers import (\n    AutoencoderKL,\n    UNet2DConditionModel,\n    ControlNetModel,\n    StableDiffusionControlNetPipeline,\n    DDIMScheduler,\n)\n\n# ---------- EDIT THESE ----------\nMODEL_DIR_SD   = \"/kaggle/input/stablediffusion5669/tensorflow2/default/1/models/sd-v1-5\"\nCHECKPOINT_DIR = \"/kaggle/working/controlnet_scratch_out/controlnet_final\"\nPROMPT         = \"eagle with mountain and some grass\"     # or the exact caption for the filename you test\nCONTROL_IMAGE  = \"/kaggle/input/canny-tuples/dataset/canny/00001.jpg\"\nOUTPUT         = \"controlnet_infer_out.png\"\n\nIMAGE_SIZE = 256            # must match training size for best fidelity\nNUM_INFERENCE_STEPS = 30    # a touch higher for scratch nets\nGUIDANCE_SCALE      = 4.0   # lower CFG lets control dominate\nSEED = 42\n# -------------------------------\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndtype  = torch.float16 if device == \"cuda\" else torch.float32\ntorch.manual_seed(SEED)\n\n# --- load SD v1.5 parts ---\ntok = CLIPTokenizer.from_pretrained(str(Path(MODEL_DIR_SD) / \"tokenizer\"))\ntxt = CLIPTextModel.from_pretrained(str(Path(MODEL_DIR_SD) / \"text_encoder\"), torch_dtype=dtype).to(device)\nvae = AutoencoderKL.from_pretrained(MODEL_DIR_SD, subfolder=\"vae\",  torch_dtype=dtype).to(device)\nunet= UNet2DConditionModel.from_pretrained(MODEL_DIR_SD, subfolder=\"unet\", torch_dtype=dtype).to(device)\n\n# --- load ControlNet ---\nif not Path(CHECKPOINT_DIR).exists():\n    raise FileNotFoundError(f\"ControlNet checkpoint not found: {CHECKPOINT_DIR}\")\nctrl = ControlNetModel.from_pretrained(CHECKPOINT_DIR, torch_dtype=dtype).to(device)\n\n# --- build pipeline ---\nscheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\npipe = StableDiffusionControlNetPipeline(\n    unet=unet, controlnet=ctrl, vae=vae, tokenizer=tok, text_encoder=txt,\n    scheduler=scheduler, safety_checker=None, feature_extractor=None\n).to(device)\n\npipe.enable_attention_slicing()\nif device == \"cuda\":\n    try:\n        import xformers  # noqa: F401\n        pipe.enable_xformers_memory_efficient_attention()\n        print(\"[ATTN] xFormers enabled.\")\n    except Exception:\n        print(\"[ATTN] xFormers not available, continuing.\")\n\n# --- control image ---\ndef load_control(img_path, size):\n    im = Image.open(img_path).convert(\"RGB\")\n    # NEAREST to preserve edge crispness; size must match training size\n    return im.resize((size, size), resample=Image.NEAREST)\n\ncontrol_img = load_control(CONTROL_IMAGE, IMAGE_SIZE)\n\n# --- run ---\ngen = torch.Generator(device=device).manual_seed(SEED)\nwith torch.autocast(device_type=\"cuda\", enabled=(device==\"cuda\")):\n    out = pipe(\n        PROMPT,\n        image=control_img,\n        num_inference_steps=NUM_INFERENCE_STEPS,\n        guidance_scale=GUIDANCE_SCALE,\n        generator=gen,\n        negative_prompt=None,\n    )\n\nout.images[0].save(OUTPUT)\nprint(f\"✅ Saved -> {OUTPUT}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T12:25:25.125295Z","iopub.execute_input":"2025-11-13T12:25:25.125620Z","iopub.status.idle":"2025-11-13T12:25:30.920012Z","shell.execute_reply.started":"2025-11-13T12:25:25.125597Z","shell.execute_reply":"2025-11-13T12:25:30.919195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}